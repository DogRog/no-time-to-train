{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add the project root to sys.path\n",
    "# We assume the notebook is located in <project_root>/notebooks/\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import the dataset classes\n",
    "from no_time_to_train.dataset.coco_ref_dataset import (\n",
    "    COCOMemoryFillCropDataset, COCORefOracleTestDataset)\n",
    "from no_time_to_train.dataset.few_shot_sampling import sample_memory_dataset\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "SHOTS = 10\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- 1. Load Test Dataset (Query Set) ---\n",
    "print(\"--- Loading Test Dataset (Query Set) ---\")\n",
    "query_set_config = {\n",
    "    \"root\": os.path.join(project_root, \"data/olive_diseases/val2017\"),\n",
    "    \"json_file\": os.path.join(project_root, \"data/olive_diseases/annotations/instances_val2017.json\"),\n",
    "    \"image_size\": 1024,\n",
    "    \"norm_img\": False,\n",
    "    \"class_split\": \"olive_diseases\",\n",
    "    \"with_query_points\": False\n",
    "}\n",
    "\n",
    "\n",
    "# --- 2. Load Support Dataset (Memory Set) ---\n",
    "print(\"\\n--- Loading Support Dataset (Reference Set) ---\")\n",
    "support_pkl_path = os.path.join(project_root, f\"work_dirs/olive_results/olive_{SHOTS}shot_seed{SEED}.pkl\")\n",
    "support_json_file = os.path.join(project_root, \"data/olive_diseases/annotations/instances_train2017.json\")\n",
    "\n",
    "# Generate the few-shot split if it doesn't exist\n",
    "if not os.path.exists(support_pkl_path):\n",
    "    print(f\"Generating few-shot split at {support_pkl_path}...\")\n",
    "    os.makedirs(os.path.dirname(support_pkl_path), exist_ok=True)\n",
    "    sample_memory_dataset(\n",
    "        json_file=support_json_file,\n",
    "        out_path=support_pkl_path,\n",
    "        memory_length=SHOTS,\n",
    "        remove_bad=True,\n",
    "        dataset=\"olive_diseases\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Found existing few-shot split at {support_pkl_path}\")\n",
    "\n",
    "support_set_config = {\n",
    "    \"root\": os.path.join(project_root, \"data/olive_diseases/train2017\"),\n",
    "    \"json_file\": support_json_file,\n",
    "    \"memory_pkl\": support_pkl_path,\n",
    "    \"class_split\": \"olive_diseases\",\n",
    "    \"image_size\": 1024,\n",
    "    \"memory_length\": SHOTS,\n",
    "    \"context_ratio\": 0.2,\n",
    "    \"norm_img\": False\n",
    "}\n",
    "\n",
    "support_set = COCOMemoryFillCropDataset(**support_set_config)\n",
    "query_set = COCORefOracleTestDataset(**query_set_config)\n",
    "\n",
    "print(len(support_set))\n",
    "print(len(query_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec65280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ALL 50 support images\n",
    "all_support_images = []\n",
    "all_support_metadata = []\n",
    "\n",
    "# support_set has length 50 (5 classes * 10 shots)\n",
    "for i in range(len(support_set)):\n",
    "    item = support_set[i]\n",
    "    \n",
    "    # Each item has 'refs_by_cat' with ONE category key\n",
    "    refs = item['refs_by_cat']\n",
    "    cat_ind = list(refs.keys())[0] # The internal category index (0, 1, 2, 3, 4)\n",
    "    \n",
    "    # Get the image tensor (1, 3, H, W)\n",
    "    # The dataset unsqueezes it to (1, 3, H, W)\n",
    "    img_tensor = refs[cat_ind]['imgs'][0] \n",
    "    \n",
    "    # Get the mask tensor (H, W) if needed for precise prompting\n",
    "    mask_tensor = refs[cat_ind]['masks']\n",
    "\n",
    "    # Convert to numpy (H, W, 3) for video construction\n",
    "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Store\n",
    "    all_support_images.append(img_np)\n",
    "    \n",
    "    # Store metadata\n",
    "    # Convert internal index back to real COCO category ID\n",
    "    real_cat_id = support_set.cat_inds_to_ids[cat_ind]\n",
    "    \n",
    "    all_support_metadata.append({\n",
    "        \"index\": i,\n",
    "        \"cat_ind\": cat_ind,\n",
    "        \"category_id\": real_cat_id,\n",
    "        \"image_id\": item['refs_by_cat'][cat_ind]['img_info'][0]['id']\n",
    "    })\n",
    "\n",
    "print(f\"Collected {len(all_support_images)} support images.\")\n",
    "print(f\"First image shape: {all_support_images[0].shape}\")\n",
    "print(f\"Metadata example: {all_support_metadata[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a support set\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_support_set(images, metadata, num_images=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f\"Cat ID: {metadata[i]['category_id']}\\nImage ID: {metadata[i]['image_id']}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    \n",
    "visualize_support_set(all_support_images, all_support_metadata, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. Get a query image\n",
    "query_idx = 0\n",
    "query_item = query_set[query_idx]\n",
    "\n",
    "# Check keys\n",
    "if 'target_img' in query_item:\n",
    "    query_img_tensor = query_item['target_img']\n",
    "elif 'refs_by_cat' in query_item:\n",
    "    q_refs = query_item['refs_by_cat']\n",
    "    q_cat_ind = list(q_refs.keys())[0]\n",
    "    query_img_tensor = q_refs[q_cat_ind]['imgs'][0] \n",
    "else:\n",
    "    raise ValueError(f\"Unknown item structure. Keys: {query_item.keys()}\")\n",
    "\n",
    "# Convert to numpy (C, H, W) -> (H, W, C)\n",
    "if isinstance(query_img_tensor, torch.Tensor):\n",
    "    query_img_np = query_img_tensor.permute(1, 2, 0).numpy()\n",
    "else:\n",
    "    query_img_np = query_img_tensor\n",
    "\n",
    "# 2. Helper to process images for video (float -> uint8)\n",
    "def to_uint8(img):\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# 3. Construct the frame sequence\n",
    "video_frames = []\n",
    "NUM_REPEATS = 1\n",
    "\n",
    "# Process support images\n",
    "support_frames = [to_uint8(img) for img in all_support_images]\n",
    "query_frame = to_uint8(query_img_np)\n",
    "\n",
    "# Append support frames repeated NUM_REPEATS times\n",
    "for _ in range(NUM_REPEATS):\n",
    "    video_frames.extend(support_frames)\n",
    "\n",
    "# Append query frame at the end\n",
    "video_frames.append(query_frame)\n",
    "\n",
    "# 4. Save video using OpenCV\n",
    "output_path = \"support_query_sequence.mp4\"\n",
    "fps = 5\n",
    "\n",
    "if len(video_frames) > 0:\n",
    "    height, width, layers = video_frames[0].shape\n",
    "    \n",
    "    # Try different codecs if one fails, strictly mp4v is often safe\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in video_frames:\n",
    "        # cv2 expects BGR\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        video.write(frame_bgr)\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Created video with {len(video_frames)} frames.\")\n",
    "    print(f\" - Support images: {len(all_support_images)} (repeated {NUM_REPEATS} times)\")\n",
    "    print(f\" - Query image: 1 (at the end)\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"No frames to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ada9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Sam3TrackerVideoModel, Sam3TrackerVideoProcessor\n",
    "from transformers.video_utils import load_video\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use bfloat16 for Ampere+ GPUs (RTX 30xx/40xx, A100) for better stability, otherwise float16\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 1. Initialize Model\n",
    "print(f\"Loading SAM3 model on {device} with {dtype}...\")\n",
    "\n",
    "# Load directly to GPU with optimized attention implementation\n",
    "model = Sam3TrackerVideoModel.from_pretrained(\n",
    "    \"facebook/sam3\",\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"flash_attention_2\" # Significant speedup for Transformers\n",
    ").to(device)\n",
    "\n",
    "processor = Sam3TrackerVideoProcessor.from_pretrained(\"facebook/sam3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the video\n",
    "video_path = \"support_query_sequence.mp4\"\n",
    "video_frames, _ = load_video(video_path)\n",
    "print(f\"Loaded video with {len(video_frames)} frames.\")\n",
    "\n",
    "# 3. Init Session\n",
    "# Passing dtype here ensures internal tensors are initialized in half-precision\n",
    "inference_session = processor.init_video_session(\n",
    "    video=video_frames,\n",
    "    inference_device=device,\n",
    "    processing_device=device,\n",
    "    dtype=dtype, \n",
    ")\n",
    "\n",
    "# 4. Add Support Prompts (Masks)\n",
    "print(\"Adding support prompts (masks)...\")\n",
    "\n",
    "# Determine target size once\n",
    "target_h, target_w = 1024, 1024\n",
    "if hasattr(processor, \"image_processor\") and hasattr(processor.image_processor, \"size\"):\n",
    "     size_conf = processor.image_processor.size\n",
    "     target_h = size_conf.get(\"height\", 1024)\n",
    "     target_w = size_conf.get(\"width\", 1024)\n",
    "\n",
    "# Use torch.inference_mode() - lower overhead than no_grad\n",
    "with torch.inference_mode():\n",
    "    for idx, meta in enumerate(all_support_metadata):\n",
    "        frame_idx = idx\n",
    "        \n",
    "        # Get ground truth mask\n",
    "        ds_item = support_set[meta['index']]\n",
    "        gt_mask = ds_item['refs_by_cat'][meta['cat_ind']]['masks']\n",
    "        \n",
    "        if isinstance(gt_mask, torch.Tensor):\n",
    "            gt_mask = gt_mask.numpy()\n",
    "        if gt_mask.ndim == 3:\n",
    "            gt_mask = gt_mask[0]\n",
    "\n",
    "        # --- OPTIMIZATION START: GPU-based Preprocessing ---\n",
    "        # Move raw mask to GPU immediately to avoid CPU processing bottlenecks\n",
    "        mask_tensor = torch.from_numpy(gt_mask).to(device)\n",
    "        \n",
    "        # Add batch/channel dims: (H, W) -> (1, 1, H, W)\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "        # Resize on GPU using PyTorch interpolate instead of OpenCV\n",
    "        if mask_tensor.shape[-2] != target_h or mask_tensor.shape[-1] != target_w:\n",
    "            mask_tensor = torch.nn.functional.interpolate(\n",
    "                mask_tensor, \n",
    "                size=(target_h, target_w), \n",
    "                mode='nearest'\n",
    "            )\n",
    "        \n",
    "        # Binarize and cast to correct dtype\n",
    "        mask_tensor = (mask_tensor > 0).to(dtype)\n",
    "        # --- OPTIMIZATION END ---\n",
    "\n",
    "        obj_id = meta['cat_ind'] + 1\n",
    "        \n",
    "        processor.add_inputs_to_inference_session(\n",
    "            inference_session=inference_session,\n",
    "            frame_idx=frame_idx,\n",
    "            obj_ids=[obj_id],\n",
    "            input_masks=mask_tensor\n",
    "        )\n",
    "        \n",
    "        # Encode prompt\n",
    "        model(\n",
    "            inference_session=inference_session,\n",
    "            frame_idx=frame_idx,\n",
    "        )\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed support frame {idx}/{len(all_support_metadata)}\")\n",
    "\n",
    "print(f\"Added prompts for {len(all_support_metadata)} support frames. Propagating...\")\n",
    "\n",
    "# 5. Propagate\n",
    "video_segments = {}\n",
    "\n",
    "# Run propagation in inference mode\n",
    "with torch.inference_mode():\n",
    "    for output in model.propagate_in_video_iterator(inference_session):\n",
    "        video_segments[output.frame_idx] = output.pred_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize Result on Query Frame\n",
    "query_frame_idx = len(video_frames) - 1\n",
    "print(f\"--- Result for Query Frame {query_frame_idx} ---\")\n",
    "\n",
    "if query_frame_idx in video_segments:\n",
    "    # Keep logits on GPU for post-processing if possible, but processor usually handles list of tensors\n",
    "    masks_logits = video_segments[query_frame_idx]\n",
    "    \n",
    "    # Post-process (This step often requires CPU sync internally in HF processors, but we delayed it until the end)\n",
    "    video_res_masks = processor.post_process_masks(\n",
    "        [masks_logits], \n",
    "        original_sizes=[[inference_session.video_height, inference_session.video_width]], \n",
    "        binarize=True\n",
    "    )[0]\n",
    "    \n",
    "    # Visualization Code (CPU side)\n",
    "    \n",
    "    # Check for Ground Truth\n",
    "    gt_anns = query_item.get('tar_anns_by_cat', None)\n",
    "    \n",
    "    if gt_anns:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        ax_pred = axes[0]\n",
    "        ax_gt = axes[1]\n",
    "    else:\n",
    "        fig, ax_pred = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax_gt = None\n",
    "\n",
    "    # --- Plot Prediction ---\n",
    "    ax_pred.imshow(query_img_np) \n",
    "    \n",
    "    found = False\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    \n",
    "    # Move final mask to CPU only when strictly needed for plotting\n",
    "    video_res_masks_np = video_res_masks.cpu().numpy()\n",
    "    \n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(video_res_masks_np.shape[0]):\n",
    "        m = video_res_masks_np[i, 0]\n",
    "        if m.max() > 0:\n",
    "            found = True\n",
    "            color = np.array(cmap(i % 10))\n",
    "            colored_mask = np.zeros((m.shape[0], m.shape[1], 4))\n",
    "            colored_mask[m > 0] = color\n",
    "            colored_mask[..., 3] = (m > 0) * 0.5 \n",
    "            ax_pred.imshow(colored_mask)\n",
    "\n",
    "            handles.append(Patch(color=color, label=f\"Object {i + 1}\"))\n",
    "            labels.append(f\"Object {i + 1}\")\n",
    "\n",
    "    if handles:\n",
    "        ax_pred.legend(handles=handles, labels=labels, loc=\"upper right\", frameon=True)\n",
    "\n",
    "    ax_pred.set_title(f\"Predicted Disease on Query Image\\n(Frame {query_frame_idx})\")\n",
    "    ax_pred.axis('off')\n",
    "    \n",
    "    if not found:\n",
    "        print(\"No disease detected on query frame.\")\n",
    "\n",
    "    # --- Plot Ground Truth ---\n",
    "    if ax_gt is not None:\n",
    "        ax_gt.imshow(query_img_np)\n",
    "        \n",
    "        gt_handles = []\n",
    "        gt_labels = []\n",
    "        \n",
    "        # Iterate over ground truth categories available in tar_anns_by_cat\n",
    "        # We try to match colors with predictions if possible.\n",
    "        # Prediction index i corresponds to cat_ind = i\n",
    "        \n",
    "        for cat_ind, ann_data in gt_anns.items():\n",
    "            # Use same color encoding as prediction: i = cat_ind\n",
    "            color = np.array(cmap(cat_ind % 10))\n",
    "            \n",
    "            gt_tensor = ann_data['masks']\n",
    "            if isinstance(gt_tensor, torch.Tensor):\n",
    "                if gt_tensor.ndim == 3:\n",
    "                     gt_mask = gt_tensor.sum(dim=0).cpu().numpy() > 0\n",
    "                else:\n",
    "                     gt_mask = gt_tensor.cpu().numpy() > 0\n",
    "            else:\n",
    "                gt_mask = gt_tensor # Assuming numpy if not tensor\n",
    "\n",
    "            if gt_mask.max() > 0:\n",
    "                colored_mask = np.zeros((gt_mask.shape[0], gt_mask.shape[1], 4))\n",
    "                colored_mask[gt_mask > 0] = color\n",
    "                colored_mask[..., 3] = (gt_mask > 0) * 0.5\n",
    "                ax_gt.imshow(colored_mask)\n",
    "\n",
    "                # Add to legend\n",
    "                real_cat_id = support_set.cat_inds_to_ids[cat_ind] if hasattr(support_set, 'cat_inds_to_ids') else f\"ind_{cat_ind}\"\n",
    "                gt_handles.append(Patch(color=color, label=f\"Class {real_cat_id} ({cat_ind})\"))\n",
    "                gt_labels.append(f\"Class {real_cat_id} ({cat_ind})\")\n",
    "\n",
    "        if gt_handles:\n",
    "            ax_gt.legend(handles=gt_handles, labels=gt_labels, loc=\"upper right\", frameon=True)\n",
    "\n",
    "        ax_gt.set_title(\"Ground Truth Annotations\")\n",
    "        ax_gt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: No prediction propagated to the last frame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate IoU against Ground Truth\n",
    "if 'video_res_masks_np' in locals():\n",
    "    print(f\"\\n--- Quantitative Evaluation ---\")\n",
    "    \n",
    "    # 1. Helper function\n",
    "    def calculate_iou(pred_mask, gt_mask):\n",
    "        intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "        union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "        if union == 0:\n",
    "            return 1.0 if intersection == 0 else 0.0 \n",
    "        return intersection / union\n",
    "\n",
    "    # 2. Get GT\n",
    "    # query_item was loaded earlier\n",
    "    gt_anns = query_item.get('tar_anns_by_cat', None)\n",
    "    \n",
    "    if gt_anns:\n",
    "        ious = []\n",
    "        \n",
    "        # video_res_masks_np has shape (N_tracked, 1, H, W)\n",
    "        # N_tracked should match the number of unique categories we prompted (or at least queried)\n",
    "        print(f\"Evaluating {video_res_masks_np.shape[0]} predictions...\")\n",
    "        \n",
    "        for i in range(video_res_masks_np.shape[0]):\n",
    "            pred_mask = video_res_masks_np[i, 0] > 0\n",
    "            \n",
    "            # Assuming implicit mapping: prediction i corresponds to obj_id i+1 -> category index i\n",
    "            # This relies on objects being tracked with IDs 1..N and returned in order\n",
    "            cat_ind = i \n",
    "            \n",
    "            gt_mask = np.zeros_like(pred_mask, dtype=bool)\n",
    "            \n",
    "            if cat_ind in gt_anns:\n",
    "                # gt_anns[cat_ind]['masks'] is typically a tensor (N_inst, H, W)\n",
    "                gt_tensor = gt_anns[cat_ind]['masks']\n",
    "                \n",
    "                # Convert to numpy and merge instances (semantic segmentation style)\n",
    "                if gt_tensor.ndim == 3:\n",
    "                     gt_encoded = gt_tensor.sum(dim=0).cpu().numpy() > 0\n",
    "                else:\n",
    "                     gt_encoded = gt_tensor.cpu().numpy() > 0\n",
    "                gt_mask = gt_encoded\n",
    "                \n",
    "            iou = calculate_iou(pred_mask, gt_mask)\n",
    "            ious.append(iou)\n",
    "            \n",
    "            # Label\n",
    "            cat_id = support_set.cat_inds_to_ids[cat_ind] if hasattr(support_set, 'cat_inds_to_ids') else f\"ind_{cat_ind}\"\n",
    "            print(f\"Class {cat_ind} (ID {cat_id}): IoU = {iou:.4f}\")\n",
    "            \n",
    "        print(f\"Mean IoU: {np.mean(ious):.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Ground Truth annotations (tar_anns_by_cat) not found in 'query_item'.\")\n",
    "        print(\"Available keys:\", query_item.keys())\n",
    "else:\n",
    "    print(\"No predictions found (video_res_masks_np not defined). Run previous cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no-time-to-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
