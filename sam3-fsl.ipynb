{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b0ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing model and processor.\n",
      "Loading COCO annotations...\n",
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "--- Evaluating Support Set v1 (1-shot) ---\n",
      "  Anthracnose: 38 query images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Anthracnose:   0%|          | 0/38 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 392\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m K_VALUES:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m v_name, v_dir \u001b[38;5;129;01min\u001b[39;00m SUPPORT_SETS_DIRS.items():\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m         \u001b[43mevaluate_support_set_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 343\u001b[39m, in \u001b[36mevaluate_support_set_version\u001b[39m\u001b[34m(version_name, support_dir, k_shot)\u001b[39m\n\u001b[32m    335\u001b[39m inputs = processor(\n\u001b[32m    336\u001b[39m     images=concat_img, \n\u001b[32m    337\u001b[39m     input_boxes=input_boxes,\n\u001b[32m    338\u001b[39m     input_boxes_labels=input_boxes_labels,\n\u001b[32m    339\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m ).to(device)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m results = processor.post_process_instance_segmentation(\n\u001b[32m    346\u001b[39m     outputs, threshold=\u001b[32m0.4\u001b[39m, mask_threshold=\u001b[32m0.5\u001b[39m,\n\u001b[32m    347\u001b[39m     target_sizes=inputs.get(\u001b[33m\"\u001b[39m\u001b[33moriginal_sizes\u001b[39m\u001b[33m\"\u001b[39m).tolist()\n\u001b[32m    348\u001b[39m )[\u001b[32m0\u001b[39m]\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Post-processing: Map masks back to original query image\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/models/sam3/modeling_sam3.py:2278\u001b[39m, in \u001b[36mSam3Model.forward\u001b[39m\u001b[34m(self, pixel_values, vision_embeds, input_ids, attention_mask, text_embeds, input_boxes, input_boxes_labels, **kwargs)\u001b[39m\n\u001b[32m   2275\u001b[39m     device = vision_embeds.fpn_hidden_states[\u001b[32m0\u001b[39m].device\n\u001b[32m   2277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vision_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2278\u001b[39m     vision_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2280\u001b[39m     vision_outputs = vision_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/models/sam3/modeling_sam3.py:1038\u001b[39m, in \u001b[36mSam3VisionModel.forward\u001b[39m\u001b[34m(self, pixel_values, **kwargs)\u001b[39m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify pixel_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m backbone_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m hidden_states = backbone_output.last_hidden_state  \u001b[38;5;66;03m# [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# Reshape for FPN neck: [batch_size, seq_len, hidden_size] -> [batch_size, hidden_size, height, width]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/models/sam3/modeling_sam3.py:826\u001b[39m, in \u001b[36mSam3ViTModel.forward\u001b[39m\u001b[34m(self, pixel_values, **kwargs)\u001b[39m\n\u001b[32m    824\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     hidden_states = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# Reshape back to sequence format: [batch_size, height*width, hidden_size]\u001b[39;00m\n\u001b[32m    829\u001b[39m hidden_states = hidden_states.view(batch_size, height * width, hidden_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/models/sam3/modeling_sam3.py:744\u001b[39m, in \u001b[36mSam3ViTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m     hidden_states, pad_height_width = window_partition(hidden_states, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m    743\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb()\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# Reverse window partition to restore original spatial layout\u001b[39;00m\n\u001b[32m    748\u001b[39m     hidden_states = window_unpartition(hidden_states, \u001b[38;5;28mself\u001b[39m.window_size, pad_height_width, (height, width))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/transformers/models/sam3/modeling_sam3.py:507\u001b[39m, in \u001b[36mSam3ViTRoPEAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m new_shape = (batch_size, seq_len, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    506\u001b[39m query = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(*new_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(*new_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    508\u001b[39m value = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(*new_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    509\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/VScode/no-time-to-train/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import pycocotools.mask as mask_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from transformers import Sam3Processor, Sam3Model\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Initialize model if not present (e.g. if previous cells weren't run)\n",
    "if 'model' not in globals() or 'processor' not in globals():\n",
    "    print(\"Model or processor not found. Initializing...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n",
    "    processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n",
    "else:\n",
    "    print(\"Using existing model and processor.\")\n",
    "    if 'device' not in globals():\n",
    "         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Metrics Classes from no-time-to-train ---\n",
    "\n",
    "def mask_to_rle(binary_mask):\n",
    "    rle = mask_utils.encode(np.asfortranarray(binary_mask))\n",
    "    rle['counts'] = rle['counts'].decode('utf-8')\n",
    "    return rle\n",
    "\n",
    "class COCOInstToSegmEvaluator:\n",
    "    def __init__(self, coco_gt, results, cat_ids=None, img_ids=None):\n",
    "        \"\"\"Initialize the evaluator with ground truth and prediction paths\"\"\"\n",
    "        self.confidence_threshold = 0.5\n",
    "        self.coco_gt = coco_gt\n",
    "        self.results = results\n",
    "        \n",
    "        if cat_ids is None:\n",
    "            self.cat_ids = sorted(coco_gt.getCatIds())\n",
    "        else:\n",
    "            self.cat_ids = sorted(cat_ids)\n",
    "            \n",
    "        # Create category id to index mapping\n",
    "        # Note: mapping starts at 1 to differentiate from background (0)\n",
    "        self.cat_id_to_idx = {cat_id: idx + 1 for idx, cat_id in enumerate(self.cat_ids)}\n",
    "        \n",
    "        # Pre-compute all semantic masks\n",
    "        self.pred_semantic_masks = {}\n",
    "        self.gt_semantic_masks = {}\n",
    "        \n",
    "        # Determine images to evaluate\n",
    "        if img_ids is None:\n",
    "            target_img_ids = coco_gt.imgs.keys()\n",
    "        else:\n",
    "            target_img_ids = img_ids\n",
    "\n",
    "        self.image_sizes = {img_id: (coco_gt.imgs[img_id]['height'], coco_gt.imgs[img_id]['width']) \n",
    "                           for img_id in target_img_ids if img_id in coco_gt.imgs}\n",
    "        \n",
    "        # Pre-compute all prediction masks\n",
    "        print(\"Converting instance predictions to semantic masks...\")\n",
    "        # Optimize: group results by image_id first\n",
    "        self.results_by_img = defaultdict(list)\n",
    "        for r in self.results:\n",
    "            self.results_by_img[r['image_id']].append(r)\n",
    "            \n",
    "        for img_id, (height, width) in tqdm(self.image_sizes.items()):\n",
    "            self.pred_semantic_masks[img_id] = self._convert_pred_to_semantic(\n",
    "                img_id, height, width)\n",
    "            \n",
    "        # Pre-compute all ground truth masks\n",
    "        print(\"Converting ground truth instances to semantic masks...\")\n",
    "        for img_id, (height, width) in tqdm(self.image_sizes.items()):\n",
    "            self.gt_semantic_masks[img_id] = self._convert_gt_to_semantic(\n",
    "                img_id, height, width)\n",
    "\n",
    "    def _convert_pred_to_semantic(self, img_id, height, width):\n",
    "        \"\"\"Helper method to convert predictions for one image\"\"\"\n",
    "        semantic_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        img_preds = [p for p in self.results_by_img.get(img_id, [])\n",
    "                    if p['score'] >= self.confidence_threshold]\n",
    "        \n",
    "        # Sort by score descending (highest first)\n",
    "        img_preds.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        for pred in img_preds:\n",
    "            if pred['category_id'] not in self.cat_id_to_idx:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(pred['segmentation'], dict):\n",
    "                 binary_mask = mask_utils.decode(pred['segmentation'])\n",
    "            else:\n",
    "                 # Helper if rle is not in dict format (unlikely with pycocotools)\n",
    "                 pass\n",
    "\n",
    "            category_idx = self.cat_id_to_idx[pred['category_id']]\n",
    "            semantic_mask[binary_mask > 0] = category_idx\n",
    "            \n",
    "        return semantic_mask\n",
    "\n",
    "    def _convert_gt_to_semantic(self, img_id, height, width):\n",
    "        \"\"\"Helper method to convert ground truth for one image\"\"\"\n",
    "        semantic_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        \n",
    "        # Load anns\n",
    "        ann_ids = self.coco_gt.getAnnIds(imgIds=img_id, iscrowd=0)\n",
    "        img_anns = self.coco_gt.loadAnns(ann_ids)\n",
    "        \n",
    "        for ann in img_anns:\n",
    "            if ann['category_id'] not in self.cat_id_to_idx:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(ann['segmentation'], dict):\n",
    "                binary_mask = mask_utils.decode(ann['segmentation'])\n",
    "            else:\n",
    "                rles = mask_utils.frPyObjects(ann['segmentation'], height, width)\n",
    "                rle = mask_utils.merge(rles)\n",
    "                binary_mask = mask_utils.decode(rle)\n",
    "            \n",
    "            category_idx = self.cat_id_to_idx[ann['category_id']]\n",
    "            semantic_mask[binary_mask > 0] = category_idx\n",
    "            \n",
    "        return semantic_mask\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate semantic segmentation results\"\"\"\n",
    "        total_inter = defaultdict(int)\n",
    "        total_union = defaultdict(int)\n",
    "        \n",
    "        print(\"Computing IoU metrics...\")\n",
    "        for img_id in self.image_sizes.keys():\n",
    "            pred_mask = torch.from_numpy(self.pred_semantic_masks[img_id])\n",
    "            gt_mask = torch.from_numpy(self.gt_semantic_masks[img_id])\n",
    "            \n",
    "            # Use computed indices (1..N)\n",
    "            for cat_id, class_idx in self.cat_id_to_idx.items():\n",
    "                pred_binary = (pred_mask == class_idx)\n",
    "                gt_binary = (gt_mask == class_idx)\n",
    "                \n",
    "                intersection = (pred_binary & gt_binary).sum().item()\n",
    "                union = (pred_binary | gt_binary).sum().item()\n",
    "                \n",
    "                total_inter[cat_id] += intersection\n",
    "                total_union[cat_id] += union\n",
    "        \n",
    "        # Compute IoU for each class (and mean)\n",
    "        ious = {}\n",
    "        sum_iou = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        ids_to_names = {cat['id']: cat['name'] for cat in self.coco_gt.loadCats(self.cat_ids)}\n",
    "\n",
    "        print(\"Per-class IoU:\")\n",
    "        for cat_id in self.cat_ids:\n",
    "            if total_union[cat_id] == 0:\n",
    "                continue\n",
    "                \n",
    "            iou = total_inter[cat_id] / total_union[cat_id]\n",
    "            ious[cat_id] = iou\n",
    "            sum_iou += iou\n",
    "            count += 1\n",
    "            print(f\"  {ids_to_names.get(cat_id, cat_id)}: {iou:.4f}\")\n",
    "        \n",
    "        miou = sum_iou / count if count > 0 else 0.0\n",
    "        print(f\"Mean IoU: {miou:.4f}\")\n",
    "        return miou, ious\n",
    "\n",
    "def evaluate_coco_metrics(coco_gt, results_list, img_ids):\n",
    "    # 1. Standard COCO Instance Segmentation Evaluation\n",
    "    print(\"\\n--- COCO Instance Segmentation Evaluation (mAP) ---\")\n",
    "    \n",
    "    # Needs a hack for COCOeval to accept existing list, usually it loads from file or directly\n",
    "    # But cocoeval usually takes a coco object for res.\n",
    "    # We can create a COCO object for results\n",
    "    coco_dt = coco_gt.loadRes(results_list)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'segm')\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "    # 2. Semantic Segmentation Evaluation (mIoU)\n",
    "    print(\"\\n--- Semantic Segmentation Evaluation (mIoU) ---\")\n",
    "    inst_seg_evaluator = COCOInstToSegmEvaluator(coco_gt, results_list, img_ids=img_ids)\n",
    "    inst_seg_evaluator.evaluate()\n",
    "\n",
    "# --- End Metrics Classes ---\n",
    "\n",
    "# Settings\n",
    "DATA_DIR = \"./data/olive_diseases\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train2017\")\n",
    "VAL_IMG_DIR = os.path.join(DATA_DIR, \"val2017\")\n",
    "VAL_ANN_FILE = os.path.join(DATA_DIR, \"annotations/instances_val2017.json\")\n",
    "TRAIN_ANN_FILE = os.path.join(DATA_DIR, \"annotations/instances_train2017.json\")\n",
    "\n",
    "SUPPORT_SETS_DIRS = {\n",
    "    \"v1\": \"support_sets_olive\",\n",
    "    \"v2\": \"support_sets_olive_v2\"\n",
    "}\n",
    "# K_SHOT = 3 # Removed single K\n",
    "K_VALUES = [1, 2, 3, 5, 10]\n",
    "# MAX_QUERY_IMAGES = 5 # Removed limit\n",
    "\n",
    "# Load COCO\n",
    "print(\"Loading COCO annotations...\")\n",
    "coco_train = COCO(TRAIN_ANN_FILE)\n",
    "coco_val = COCO(VAL_ANN_FILE)\n",
    "cats = coco_val.loadCats(coco_val.getCatIds())\n",
    "cat_id_to_name = {cat['id']: cat['name'] for cat in cats}\n",
    "\n",
    "def load_support_set(pkl_path):\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_image(coco, img_dir, img_id):\n",
    "    img_info = coco.loadImgs([img_id])[0]\n",
    "    img_path = os.path.join(img_dir, img_info['file_name'])\n",
    "    return Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "def get_ann_bboxes(coco, ann_ids):\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    bboxes = []\n",
    "    for ann in anns:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        bboxes.append([x, y, x + w, y + h]) # Convert to xyxy\n",
    "    return bboxes\n",
    "\n",
    "def concat_images_and_boxes(support_data_list, query_image, target_h=512):\n",
    "    # support_data_list: list of (image, bboxes)\n",
    "    # bboxes are [x1, y1, x2, y2]\n",
    "    \n",
    "    resized_imgs = []\n",
    "    all_shifted_bboxes = []\n",
    "    \n",
    "    current_x = 0\n",
    "    \n",
    "    # Process support images\n",
    "    for img, bboxes in support_data_list:\n",
    "        w, h = img.size\n",
    "        scale = target_h / h\n",
    "        new_w = int(w * scale)\n",
    "        img_resized = img.resize((new_w, target_h), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Transform bboxes\n",
    "        for box in bboxes:\n",
    "            bx1, by1, bx2, by2 = box\n",
    "            # Scale\n",
    "            bx1 *= scale\n",
    "            by1 *= scale\n",
    "            bx2 *= scale\n",
    "            by2 *= scale\n",
    "            # Shift\n",
    "            bx1 += current_x\n",
    "            bx2 += current_x\n",
    "            # Append\n",
    "            all_shifted_bboxes.append([bx1, by1, bx2, by2])\n",
    "            \n",
    "        resized_imgs.append(img_resized)\n",
    "        current_x += new_w\n",
    "        \n",
    "    # Process query\n",
    "    w, h = query_image.size\n",
    "    scale = target_h / h\n",
    "    query_new_w = int(w * scale)\n",
    "    query_resized = query_image.resize((query_new_w, target_h), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create canvas\n",
    "    final_w = current_x + query_new_w\n",
    "    final_img = Image.new(\"RGB\", (final_w, target_h))\n",
    "    \n",
    "    # Paste support\n",
    "    curr_paste_x = 0\n",
    "    for img in resized_imgs:\n",
    "        final_img.paste(img, (curr_paste_x, 0))\n",
    "        curr_paste_x += img.size[0]\n",
    "        \n",
    "    # Paste query\n",
    "    query_x_start = curr_paste_x\n",
    "    final_img.paste(query_resized, (query_x_start, 0))\n",
    "    query_x_end = query_x_start + query_new_w\n",
    "    \n",
    "    # Return image, support bboxes in new image coord, query bbox in new image coord\n",
    "    return final_img, all_shifted_bboxes, (query_x_start, 0, query_x_end, target_h)\n",
    "\n",
    "def evaluate_support_set_version(version_name, support_dir, k_shot):\n",
    "    print(f\"\\n--- Evaluating Support Set {version_name} ({k_shot}-shot) ---\")\n",
    "    pkl_path = os.path.join(support_dir, f\"olive_diseases_{k_shot}shot.pkl\")\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"File not found: {pkl_path}\")\n",
    "        return\n",
    "    \n",
    "    support_data = load_support_set(pkl_path)\n",
    "    \n",
    "    all_results = []\n",
    "    evaluated_img_ids = set()\n",
    "    \n",
    "    for cat_id, cat_name in cat_id_to_name.items():\n",
    "        if cat_id not in support_data: continue\n",
    "        \n",
    "        # Prepare Support Data\n",
    "        supports = support_data[cat_id] # List of dicts\n",
    "        support_items = []\n",
    "        for item in supports:\n",
    "            s_img = get_image(coco_train, TRAIN_IMG_DIR, item['img_id'])\n",
    "            # Get bboxes for the few-shot examples\n",
    "            s_bboxes = get_ann_bboxes(coco_train, item['ann_ids'])\n",
    "            support_items.append((s_img, s_bboxes))\n",
    "            \n",
    "        # Get query images (ALL images now)\n",
    "        query_img_ids = coco_val.getImgIds(catIds=[cat_id])\n",
    "        print(f\"  {cat_name}: {len(query_img_ids)} query images\")\n",
    "        \n",
    "        for q_img_id in tqdm(query_img_ids, desc=f\"Evaluating {cat_name}\"):\n",
    "            evaluated_img_ids.add(q_img_id)\n",
    "            q_img = get_image(coco_val, VAL_IMG_DIR, q_img_id)\n",
    "            \n",
    "            # Concatenate\n",
    "            concat_img, input_bboxes, query_bbox = concat_images_and_boxes(support_items, q_img)\n",
    "            \n",
    "            # Correct unpacking: query_bbox is (q_x1, q_y1, q_x2, q_y2)\n",
    "            q_x1, q_y1, q_x2, q_y2 = query_bbox\n",
    "            \n",
    "            if not input_bboxes:\n",
    "                continue\n",
    "\n",
    "            # Run SAM3 with BBox prompts\n",
    "            input_boxes = [input_bboxes] \n",
    "            input_boxes_labels = [[1] * len(input_bboxes)] \n",
    "            \n",
    "            inputs = processor(\n",
    "                images=concat_img, \n",
    "                input_boxes=input_boxes,\n",
    "                input_boxes_labels=input_boxes_labels,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            results = processor.post_process_instance_segmentation(\n",
    "                outputs, threshold=0.4, mask_threshold=0.5,\n",
    "                target_sizes=inputs.get(\"original_sizes\").tolist()\n",
    "            )[0]\n",
    "            \n",
    "            # Post-processing: Map masks back to original query image\n",
    "            masks = results['masks'].cpu().numpy()\n",
    "            scores = results['scores'].cpu().numpy()\n",
    "            \n",
    "            # Original image size\n",
    "            orig_w, orig_h = q_img.size\n",
    "            # The query part in concat image has size (q_x2-q_x1, q_y2-q_y1)\n",
    "            # which corresponds to scaled version of original image.\n",
    "            \n",
    "            for mask, score in zip(masks, scores):\n",
    "                # Crop the mask to the query image area\n",
    "                mask_crop = mask[q_y1:q_y2, q_x1:q_x2]\n",
    "                \n",
    "                # If mask is empty in query area, skip\n",
    "                if mask_crop.sum() == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Resize back to original dimensions\n",
    "                # mask_crop is (target_h, new_w) - we want (orig_h, orig_w)\n",
    "                # cv2.resize takes (width, height)\n",
    "                mask_orig = cv2.resize(mask_crop.astype(np.uint8), (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "                \n",
    "                # Convert to RLE\n",
    "                rle = mask_to_rle(mask_orig)\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"image_id\": q_img_id,\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"segmentation\": rle,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "    # Run Standard Evaluation\n",
    "    if all_results:\n",
    "        print(f\"Running metrics for {k_shot}-shot on {len(evaluated_img_ids)} images...\")\n",
    "        evaluate_coco_metrics(coco_val, all_results, sorted(list(evaluated_img_ids)))\n",
    "    else:\n",
    "        print(\"No predictions generated.\")\n",
    "\n",
    "# Run Evaluations\n",
    "for k in K_VALUES:\n",
    "    for v_name, v_dir in SUPPORT_SETS_DIRS.items():\n",
    "        evaluate_support_set_version(v_name, v_dir, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0d918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "no-time-to-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
